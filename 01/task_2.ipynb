{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment: Neural network basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft deadline: 16.09.18 at 23.59\n",
    "\n",
    "Hard deadline: 18.09.18 at 23.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task I intentionally provide no boilerplate code, because very puprpose of this task is getting you comforatable with basic code template for desiging NNs in pytorch. I higly recommend you to revisit all the last seminar materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "* Implement simple **fully-convolutional** neural architecture for classification. Make sure it is small enought to run on your home machine.\n",
    "* Provide dataset visulization.\n",
    "* Provide train/test split and validation\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Architecture should derive from `torch.nn.Module`\n",
    "* Use `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`. But if you manage co simplify this step using dataset `torchivision`, I will only encourage you.\n",
    "* Implement at least one data transformer, but make sure it is useful for classification task.\n",
    "* Use FashionMNIST dataset https://github.com/zalandoresearch/fashion-mnist\n",
    "* Make sure you can fix random seed for all components of your code to make experiments reproducible\n",
    "* Since you architecure should be fully-convolutional, make sure it does not depend on input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 100\n",
    "learing_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"fasion_mnist\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "train_dataset = datasets.FashionMNIST(dataset_dir, train=True, download=False, transform=transform)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(dataset_dir, train=False, download=False, transform=transform)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000, -0.8980, -0.4275, -1.0000, -1.0000, -0.9922, -0.9686,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -0.9922, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -1.0000,\n          -0.7176,  0.0667, -0.0039, -0.5137, -0.5765, -1.0000, -1.0000,\n          -1.0000, -0.9922, -0.9765, -0.9686, -1.0000, -1.0000, -0.9765],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9529, -1.0000,\n          -0.2000,  0.6000,  0.3804,  0.0510,  0.1294, -0.0353, -0.8196,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9059, -0.9216, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.2157,  0.8510,  0.6235,  0.3961, -0.1608,  0.2235,  0.2627,\n          -0.1451, -0.4980, -0.8196, -0.3961,  0.0196, -0.4353, -0.8824],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000, -0.4588,\n           0.6235,  0.7490,  0.7098,  0.6941,  0.6941,  0.2784, -0.0039,\n          -0.0510, -0.0431,  0.1451,  0.1059, -0.3098,  0.3490, -0.4824],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.9922, -0.9922, -0.9922, -1.0000,  0.5686,\n           0.8196,  0.8196,  0.8275,  0.7961,  0.7490,  0.7490,  0.6863,\n           0.6706,  0.2863, -0.0039, -0.0353,  0.5373,  0.7961, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4353,\n           0.7647,  0.6941,  0.7490,  0.7882,  0.8431,  0.7804,  0.7569,\n           0.7412,  0.7569,  0.7333,  0.7490,  0.9216,  0.3569, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5137,\n           0.7882,  0.7098,  0.6706,  0.5529,  0.4118,  0.6627,  0.6471,\n           0.6549,  0.6706,  0.7490,  0.7255,  0.9059,  0.5843, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.9922, -0.9765, -1.0000, -0.9059,  0.7176,\n           0.7255,  0.6627,  0.7098,  0.5059,  0.3255,  0.7804,  0.6314,\n           0.7098,  0.7569,  0.6627,  0.7725,  0.5451,  0.6392, -0.5922],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.9529, -1.0000, -0.2235,  0.9137,\n           0.7412,  0.7255,  0.7098,  0.5922,  0.5529,  0.7333,  0.6863,\n           0.6706,  0.7412,  0.7255,  0.9216, -0.0667,  0.3098, -0.5608],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.9686, -1.0000, -1.0000, -0.5686,  0.8510,\n           0.7882,  0.8039,  0.7882,  0.8824,  0.8196,  0.6706,  0.7098,\n           0.7490,  0.8353,  0.7020,  0.7020,  0.6392, -0.2784, -1.0000],\n         [-1.0000, -1.0000, -0.9922, -0.9686, -0.9529, -0.9451, -0.9843,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.8588,  0.7725,\n           0.7020,  0.7490,  0.7412,  0.7176,  0.7412,  0.7333,  0.6941,\n           0.7490,  0.7961,  0.6863,  0.7098,  1.0000, -0.3961, -1.0000],\n         [-1.0000, -0.9765, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.5137,  0.1373,  0.6000,  0.7882,  0.6235,\n           0.6706,  0.7333,  0.7098,  0.6314,  0.6549,  0.7098,  0.7569,\n           0.7490,  0.7176,  0.6863,  0.7569,  0.9137,  0.2471, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.8588, -0.6549, -0.3569,\n          -0.1608,  0.4824,  0.7882,  0.7255,  0.7412,  0.7020,  0.7725,\n           0.5686,  0.6078,  0.6549,  0.8039,  0.7569,  0.8353,  0.3804,\n           0.4745,  0.9608,  0.9451,  0.8275,  0.8667,  0.6863, -1.0000],\n         [-1.0000, -0.5529,  0.4667,  0.6314,  0.7569,  0.7333,  0.7569,\n           0.6314,  0.6000,  0.6784,  0.6314,  0.6392,  0.5686,  0.2471,\n           0.9216,  0.5137,  0.6157,  0.7490,  1.0000,  1.0000,  0.7333,\n           0.8353,  0.7333,  0.6549,  0.7255,  0.8196,  0.9294, -1.0000],\n         [-0.9765,  0.5843,  0.7882,  0.7569,  0.7333,  0.6549,  0.6549,\n           0.6784,  0.6078,  0.6078,  0.6078,  0.7255,  0.8824, -0.3725,\n           0.1765,  1.0000,  0.7961,  0.7333,  0.4745,  0.2078,  0.4980,\n           0.6471,  0.6000,  0.6392,  0.7412,  0.7882,  0.7647, -1.0000],\n         [-0.2314,  0.8275,  0.5529,  0.6471,  0.7412,  0.7961,  0.7961,\n           0.8353,  0.9529,  0.7255,  0.5216,  0.6863,  0.7020,  0.8902,\n          -0.4902, -0.4275, -0.1686, -0.0824,  0.3176,  0.7176,  0.7333,\n           0.6863,  0.7020,  0.7490,  0.7490,  0.7569,  0.7961, -0.7725],\n         [-0.4118,  0.6000,  0.6627,  0.6000,  0.5137,  0.6078,  0.6549,\n           0.7647,  0.6941,  0.4510,  0.5451,  0.6157,  0.5529,  0.6706,\n           0.8824,  0.5294,  0.7804,  0.9216,  0.8745,  0.7490,  0.7098,\n           0.6627,  0.6392,  0.7412,  0.7255,  0.7333,  0.8039, -0.4745],\n         [-0.6235,  0.5922,  0.4353,  0.5216,  0.6706,  0.5451,  0.4510,\n           0.4902,  0.5216,  0.5059,  0.5843,  0.6784,  0.7176,  0.7333,\n           0.7255,  0.8510,  0.7647,  0.6941,  0.5608,  0.6157,  0.4588,\n           0.4196,  0.3882,  0.3490,  0.4196,  0.6078,  0.6157, -0.0980],\n         [-1.0000, -0.0431,  0.7176,  0.5137,  0.4039,  0.3412,  0.4353,\n           0.5373,  0.6000,  0.6471,  0.6706,  0.6235,  0.6549,  0.6471,\n           0.5686,  0.5373,  0.5216,  0.4980,  0.5294,  0.4980,  0.5529,\n           0.5059,  0.3804,  0.2235,  0.3098,  0.3882,  0.6471, -0.2784],\n         [-1.0000, -1.0000, -0.4196,  0.4824,  0.6627,  0.4980,  0.3725,\n           0.3490,  0.3725,  0.4196,  0.4510,  0.4745,  0.4824,  0.4745,\n           0.5137,  0.5529,  0.6000,  0.6392,  0.6471,  0.6471,  0.6549,\n           0.4745,  0.4745,  0.5216,  0.5059,  0.6941,  0.3333, -1.0000],\n         [-0.9843, -1.0000, -1.0000, -1.0000, -0.4824,  0.5686,  0.7412,\n           0.8588,  0.8745,  0.8980,  0.9294,  0.9059,  0.9137,  0.7333,\n           0.7255,  0.5137,  0.4980,  0.4039,  0.4275,  0.4275,  0.4196,\n           0.3804,  0.3020,  0.3176, -0.2235, -0.5451, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6863, -0.5216, -0.6549, -0.4353, -0.6784, -0.7255, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.6784,  0.4745, -0.1922, -0.5765,\n          -0.6235, -0.6627, -0.3176,  0.3176,  0.0431, -0.8745, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.9922, -1.0000, -1.0000, -1.0000,\n          -0.6157,  0.0667,  0.7176,  0.6941,  0.7882,  0.8510,  1.0000,\n           1.0000,  1.0000,  1.0000,  0.7020,  0.6863,  0.9922,  0.8118,\n           0.2549, -0.6471, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8902,  0.3804,\n           0.7412,  0.7569,  0.6627,  0.5922,  0.5529,  0.5373,  0.5686,\n           0.6863,  0.6000,  0.5843,  0.5765,  0.5765,  0.5765,  0.6392,\n           0.7098,  0.7569,  0.2863, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4745,  0.7176,\n           0.5686,  0.5529,  0.5843,  0.5529,  0.5608,  0.5608,  0.5765,\n           0.5373,  0.5529,  0.5529,  0.5686,  0.5686,  0.5686,  0.5686,\n           0.5765,  0.5686,  0.7647, -0.6784, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.6000,  0.7176,  0.5608,\n           0.5922,  0.5922,  0.6627,  0.8667,  0.9451,  0.9608,  0.9216,\n           0.9529,  0.9294,  0.9373,  0.9765,  0.9451,  0.8431,  0.6235,\n           0.5922,  0.5922,  0.7412,  0.0980, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.0902,  0.7725,  0.6157,\n           0.6000,  0.6235,  0.6000, -0.2078, -0.4118, -0.6314, -0.4275,\n          -0.6235, -0.6078, -0.6471, -0.6000, -0.5059, -0.1137,  0.7412,\n           0.5843,  0.6157,  0.7255,  0.7569, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.5686,  0.7412,  0.6392,\n           0.5922,  0.6863,  0.5686, -1.0000, -0.4510, -0.2314, -1.0000,\n          -0.1922, -0.5373, -0.4667, -0.4431, -0.6157, -1.0000,  0.7176,\n           0.6157,  0.6784,  0.6471,  0.9608, -0.7020, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.9373,  0.7098,  0.6627,\n           0.6471,  0.6863,  0.6784, -1.0000,  0.9922,  0.9059,  0.0902,\n           1.0000,  0.3647,  0.9686,  1.0000,  0.6078, -1.0000,  0.6863,\n           0.7020,  0.6784,  0.6314,  0.7255, -0.2549, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.6471,  0.7725,  0.6784,  0.6784,\n           0.6863,  0.7569,  0.6078, -1.0000, -0.6706, -0.7255, -0.5294,\n          -0.8745, -0.8667, -0.9059, -0.8980, -0.4510, -1.0000,  0.4824,\n           0.6941,  0.6627,  0.6157,  0.6627,  0.2235, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2863,  0.8431,  0.6784,  0.6549,\n           0.7255,  0.6941,  0.5765, -0.5922, -0.4431, -0.3020, -0.2627,\n          -0.3490, -0.3882, -0.4510, -0.4039, -0.2784, -0.3176,  0.6157,\n           0.6235,  0.7412,  0.6706,  0.7176,  0.6314, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.1686,  0.4667,  0.7490,  0.8588,\n           0.9451,  0.6549,  0.5529,  0.9765,  0.9608,  0.9451,  0.9216,\n           0.9451,  0.9765,  0.9843,  0.9608,  0.9765,  0.8745,  0.5765,\n           0.6627,  0.7647,  0.6863,  0.5137, -0.1137, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8667, -0.5765,\n           0.2471,  0.7412,  0.5137,  0.6314,  0.5059,  0.5451,  0.5686,\n           0.5686,  0.5686,  0.5686,  0.5765,  0.5922,  0.5294,  0.6471,\n           0.2941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6314,  0.7647,  0.5059,  0.6784,  0.5922,  0.6157,  0.6000,\n           0.6000,  0.6078,  0.6157,  0.6000,  0.6627,  0.5451,  0.7098,\n          -0.1608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -0.9529, -1.0000,\n          -0.6392,  0.6627,  0.5294,  0.6627,  0.5843,  0.6157,  0.6078,\n           0.6000,  0.6078,  0.6157,  0.6000,  0.6627,  0.5686,  0.7098,\n          -0.2863, -1.0000, -0.9765, -0.9922, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -0.9137,  0.5451,  0.5608,  0.6078,  0.5843,  0.6078,  0.6157,\n           0.6000,  0.6078,  0.6235,  0.6000,  0.6078,  0.6078,  0.7098,\n          -0.3961, -1.0000, -0.9608, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -1.0000,\n          -0.9843,  0.4980,  0.5529,  0.5765,  0.6078,  0.6157,  0.6078,\n           0.6078,  0.6157,  0.6392,  0.6157,  0.5608,  0.6392,  0.7176,\n          -0.4196, -1.0000, -0.9608, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843, -1.0000,\n          -1.0000,  0.4745,  0.5451,  0.5686,  0.6235,  0.6235,  0.6000,\n           0.6235,  0.6235,  0.6471,  0.6314,  0.5529,  0.6235,  0.7333,\n          -0.4353, -1.0000, -0.9686, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843, -1.0000,\n          -1.0000,  0.6863,  0.5529,  0.5922,  0.6157,  0.6314,  0.6078,\n           0.6235,  0.6235,  0.6471,  0.6314,  0.5686,  0.5843,  0.7412,\n          -0.4118, -1.0000, -0.9686, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000,  0.6627,  0.5529,  0.6392,  0.6157,  0.6392,  0.6157,\n           0.6314,  0.6235,  0.6549,  0.6157,  0.6078,  0.5529,  0.7333,\n          -0.3725, -1.0000, -0.9765, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000,  0.6000,  0.5765,  0.6078,  0.6314,  0.6235,  0.6078,\n           0.6549,  0.6078,  0.6471,  0.6471,  0.6392,  0.5294,  0.7333,\n          -0.2471, -1.0000, -0.9765, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000,  0.5843,  0.5765,  0.6078,  0.6392,  0.6235,  0.6078,\n           0.6706,  0.6157,  0.6471,  0.6392,  0.6471,  0.5216,  0.7020,\n          -0.1765, -1.0000, -0.9843, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000,  0.6000,  0.6000,  0.6078,  0.6314,  0.6235,  0.6078,\n           0.6863,  0.6235,  0.6471,  0.6314,  0.6549,  0.5137,  0.6706,\n          -0.0980, -1.0000, -0.9843, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.6000,  0.6235,  0.6235,  0.6314,  0.6157,  0.6157,\n           0.6863,  0.6471,  0.6471,  0.6235,  0.6627,  0.5294,  0.6471,\n          -0.0745, -1.0000, -0.9843, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000,  0.5529,  0.6314,  0.6314,  0.6314,  0.6000,  0.6235,\n           0.6627,  0.6627,  0.6471,  0.6235,  0.6549,  0.5373,  0.6235,\n          -0.0510, -1.0000, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000,  0.5529,  0.6471,  0.6235,  0.6314,  0.6157,  0.6392,\n           0.6706,  0.6627,  0.6549,  0.6235,  0.6471,  0.5451,  0.6235,\n          -0.0275, -1.0000, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.3490,  0.6471,  0.5922,  0.5765,  0.5608,  0.6000,\n           0.6235,  0.6078,  0.6000,  0.5765,  0.6078,  0.5451,  0.6157,\n          -0.0039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.4745,  0.7333,  0.6784,  0.8353,  0.8510,  0.8667,\n           0.9137,  0.9137,  0.9137,  0.8824,  0.9059,  0.6784,  0.7569,\n           0.2706, -1.0000, -0.9843, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000,\n          -1.0000,  0.0902,  0.1451,  0.0196,  0.0588,  0.0588,  0.0745,\n          -0.0196, -0.0275, -0.0196, -0.0510, -0.0667, -0.1059,  0.0196,\n          -0.4039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.8275, -0.0745, -0.8118, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.6235, -0.3098, -0.9608, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.9059, -0.2157,  0.6627,  0.6078,  0.4510,  0.4039,\n           0.3569,  0.4588,  0.5137,  0.7333,  0.1137, -0.3333, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.3333, -0.4039,  0.5608,  0.7647,  0.9451,\n           1.0000,  0.8667,  0.7725,  0.2314, -0.4667, -0.3725, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.2863, -0.4588, -0.2863,  0.5765,  0.7098,\n           0.7647,  0.6392,  0.2392, -0.5216, -0.2706, -0.4353, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.3804, -0.3020, -0.5216, -0.5373, -0.3176,\n          -0.1529, -0.4118, -0.5608, -0.4039, -0.2392, -0.4275, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.4118, -0.3020, -0.3725, -0.3725, -0.4745,\n          -0.5059, -0.4275, -0.3490, -0.3725, -0.2471, -0.4353, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.3961, -0.3098, -0.3961, -0.3725, -0.3490,\n          -0.3490, -0.3490, -0.3490, -0.3647, -0.2549, -0.4039, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.3020, -0.2471, -0.3725, -0.3490, -0.3647,\n          -0.3412, -0.3333, -0.3333, -0.3333, -0.2392, -0.3412, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.2706, -0.2392, -0.3647, -0.3333, -0.3412,\n          -0.3333, -0.3176, -0.3098, -0.3412, -0.2235, -0.3176, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.2549, -0.3176, -0.3412, -0.3176, -0.3098,\n          -0.3333, -0.3176, -0.3176, -0.3412, -0.2784, -0.3176, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.2392, -0.3176, -0.3176, -0.3333, -0.3098,\n          -0.3176, -0.3176, -0.3176, -0.3098, -0.3333, -0.1608, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8667, -0.2157, -0.3098, -0.3176, -0.3176, -0.3098,\n          -0.3176, -0.3176, -0.3333, -0.3020, -0.3961, -0.0745, -0.9373,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.9216, -0.2706, -0.3176, -0.3176, -0.3176, -0.3176,\n          -0.3176, -0.3098, -0.3176, -0.3020, -0.3725, -0.1922, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.9294, -0.2471, -0.3176, -0.3176, -0.3176, -0.3176,\n          -0.3176, -0.3098, -0.3176, -0.3098, -0.3176, -0.1922, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.9059, -0.2471, -0.3333, -0.3176, -0.3176, -0.3176,\n          -0.3333, -0.3176, -0.3176, -0.3098, -0.3020, -0.2157, -0.9843,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8431, -0.2549, -0.3412, -0.3098, -0.3333, -0.3176,\n          -0.3098, -0.3098, -0.3098, -0.3020, -0.3098, -0.2235, -0.9373,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8353, -0.2471, -0.3333, -0.3176, -0.3333, -0.3098,\n          -0.3098, -0.3098, -0.3098, -0.3020, -0.3020, -0.2235, -0.9216,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8118, -0.2471, -0.3333, -0.3176, -0.3333, -0.3176,\n          -0.3098, -0.3098, -0.3020, -0.3098, -0.2863, -0.2000, -0.8902,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8039, -0.2706, -0.3412, -0.3098, -0.3176, -0.3176,\n          -0.3176, -0.3176, -0.3176, -0.3020, -0.2863, -0.1922, -0.7725,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.7647, -0.2549, -0.3333, -0.3098, -0.3098, -0.3176,\n          -0.3176, -0.3176, -0.3176, -0.3020, -0.3098, -0.2000, -0.7098,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.7333, -0.2471, -0.3098, -0.3176, -0.3176, -0.3176,\n          -0.3176, -0.3176, -0.3176, -0.3333, -0.3333, -0.2392, -0.7020,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.6863, -0.2471, -0.3176, -0.3333, -0.3176, -0.3176,\n          -0.3176, -0.3176, -0.3176, -0.3333, -0.3412, -0.2784, -0.6157,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.6392, -0.2549, -0.3490, -0.3412, -0.3176, -0.3176,\n          -0.3176, -0.3176, -0.3176, -0.3176, -0.3412, -0.3176, -0.3412,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.4353, -0.2549, -0.3333, -0.3412, -0.3333, -0.3098,\n          -0.3176, -0.3176, -0.3020, -0.3176, -0.3333, -0.3490, -0.5059,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.4980, -0.2157, -0.3412, -0.3176, -0.3098, -0.3333,\n          -0.3098, -0.3098, -0.3412, -0.3176, -0.3490, -0.2549, -0.5843,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.9216, -0.2000, -0.2157, -0.2863, -0.2863, -0.3020,\n          -0.3333, -0.3412, -0.3412, -0.3176, -0.1529, -0.1686, -0.8902,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.9373, -0.4275, -0.2706, -0.1843, -0.1608,\n          -0.1922, -0.1922, -0.1686, -0.2000, -0.4118, -0.9216, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.9922, -1.0000, -1.0000, -1.0000, -0.8588, -0.6706,\n          -0.5529, -0.5608, -0.7490, -0.9373, -1.0000, -1.0000, -0.9922,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a, _ in train_dataset:\n",
    "    i += 1\n",
    "    if i > 3:\n",
    "        break\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, size_n=28, size_m=28, classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        kernel = 5\n",
    "        conv_channels_1 = 4\n",
    "        conv_channels_2 = 10\n",
    "        hidden_layer_size = 15\n",
    "        max_pool_size = 2\n",
    "        \n",
    "        self.size_n = size_n\n",
    "        self.size_m = size_m\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv_channels_1, kernel_size=kernel),\n",
    "            nn.MaxPool2d(max_pool_size)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(conv_channels_1, conv_channels_2, kernel_size=5),\n",
    "            nn.MaxPool2d(max_pool_size)\n",
    "        )\n",
    "        size_n = (size_n - kernel + 1) // max_pool_size\n",
    "        size_n = (size_n - kernel + 1) // max_pool_size\n",
    "        \n",
    "        size_m = (size_m - kernel + 1) // max_pool_size\n",
    "        size_m = (size_m - kernel + 1) // max_pool_size\n",
    "        \n",
    "        self.lin1 = nn.Linear(size_n * size_m * conv_channels_2, hidden_layer_size)\n",
    "        self.lin2 = nn.Linear(hidden_layer_size, classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, x[0].numel())\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.2267 Epoch[1/12]\noutput tensor([  -5.4231,   13.8346,  -13.8492,   -0.1907,   -5.0315,  -48.7943,\n          -8.6751, -119.0128,  -19.3955,  -61.2973], grad_fn=<SelectBackward>), class = 1\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.4204 Epoch[2/12]\noutput tensor([ -1.7768,  -8.8237,   2.2706,   0.5468,   7.3278, -30.1592,   5.2356,\n        -82.9666,  -8.9607, -28.4215], grad_fn=<SelectBackward>), class = 4\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.3033 Epoch[3/12]\noutput tensor([-21.0391, -30.4888, -21.3245, -13.9192, -21.8676,   9.0385, -21.4027,\n          9.3195, -21.0452,  17.0950], grad_fn=<SelectBackward>), class = 9\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.4166 Epoch[4/12]\noutput tensor([ -6.2629,  -6.8157,  -7.1801,   1.7272,  -5.2375, -23.6075,  -5.4261,\n        -32.3824,  -6.8254, -30.4557], grad_fn=<SelectBackward>), class = 3\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.4347 Epoch[5/12]\noutput tensor([ -9.6818, -80.8140, -30.9937, -12.8696, -70.1078,  -1.1379, -34.2184,\n          4.7902,  -3.1786,   0.9564], grad_fn=<SelectBackward>), class = 7\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.2521 Epoch[6/12]\noutput tensor([ -2.6444, -10.9288,   5.2266,  -3.1994,   7.3256, -23.2144,  -1.1065,\n        -76.3842,  -2.2833, -30.1975], grad_fn=<SelectBackward>), class = 4\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.2638 Epoch[7/12]\noutput tensor([-12.6346,  -9.5716, -18.8632, -24.8771, -12.8092, -27.4570, -19.0371,\n        -20.8190,   6.7958, -19.1721], grad_fn=<SelectBackward>), class = 8\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.2978 Epoch[8/12]\noutput tensor([ -5.0501,  -6.0155,  -9.0516, -10.2223,  -7.6716,  -7.8754,  -6.6119,\n         -9.2585,   0.7243,  -3.9627], grad_fn=<SelectBackward>), class = 8\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.3561 Epoch[9/12]\noutput tensor([ -11.3860, -108.1257,  -33.0875,  -14.3126,  -93.8093,    2.1155,\n         -45.5865,    6.6427,   -5.6213,    3.5228], grad_fn=<SelectBackward>), class = 7\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.3425 Epoch[10/12]\noutput tensor([ -6.1941,   2.1166,  -6.3468,  -5.9938,  -8.2885, -13.9411,  -8.2050,\n        -83.7117, -16.3695,  -8.0031], grad_fn=<SelectBackward>), class = 1\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.1825 Epoch[11/12]\noutput tensor([ -4.1333,  -4.5839,  -7.9067,   2.7083,  -4.7387, -19.9461,  -5.0546,\n        -23.6577,  -6.1365, -26.9207], grad_fn=<SelectBackward>), class = 3\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :0.2576 Epoch[12/12]\noutput tensor([ -13.8051, -107.7575,  -33.1344,  -10.4387,  -96.4540,    0.8927,\n         -50.1888,    6.2305,   -8.0660,    4.3104], grad_fn=<SelectBackward>), class = 7\n\n"
     ]
    }
   ],
   "source": [
    "iters = 12\n",
    "\n",
    "def train():\n",
    "    model = net.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learing_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(1, iters + 1):\n",
    "        for barch_id, (image, label) in enumerate(train_dataloader):\n",
    "            label, image = label.to(device), image.to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if barch_id % 1000 == 0:\n",
    "                print('Loss :{:.4f} Epoch[{}/{}]'.format(loss.item(), epoch, iters))\n",
    "                print(\"output {}, class = {}\".format(output[0], label[0]))\n",
    "                print()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 85.65 %\n"
     ]
    }
   ],
   "source": [
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for image, label in test_dataloader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            outputs = model(image)\n",
    "            predicted = torch.argmax(outputs,dim=1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "        print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
